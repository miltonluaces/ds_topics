{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.ml.feature as pmf\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(\"local\", \"sqlContext\")\n",
    "sql = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[6,8,13,16],[...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = sql.createDataFrame([(0.0, \"Hi I heard about Spark\"), (0.0, \"I wish Java could use case classes\"), (1.0, \"Logistic regression models are neat\")], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = pmf.Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "words = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "features = hashingTF.transform(words)\n",
    "features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(20, {6: 0.2877, 8: 0.6931, 13: 0.2877, 16: 0.5754}))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = pmf.IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(features)\n",
    "rescaledData = idfModel.transform(features)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.08372311890125275,-0.03666453883051873,0.027572209388017657]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.03136832533138139,0.026593888444559913,-0.049365587665566375]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.02151056742295623,-0.008449985273182392,-0.012495249882340432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = sql.createDataFrame([(\"Hi I heard about Spark\".split(\" \"), ), (\"I wish Java could use case classes\".split(\" \"), ), (\"Logistic regression models are neat\".split(\" \"), ) ], [\"text\"])\n",
    "\n",
    "w2v = pmf.Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "w2v_fit = w2v.fit(doc)\n",
    "res = w2v_fit.transform(doc)\n",
    "\n",
    "for row in res.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, words=['a', 'b', 'c'], features=SparseVector(3, {0: 1.0, 1: 1.0, 2: 1.0}))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sql.createDataFrame([(0, \"a b c\".split(\" \")), (1, \"a b b c a\".split(\" \"))], [\"id\", \"words\"])\n",
    "cv = pmf.CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "cv_fit = cv.fit(df)\n",
    "\n",
    "res = cv_fit.transform(df)\n",
    "res.show(truncate=False)\n",
    "res.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Hasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|real|bool |stringNum|string|features                                                |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "|2.2 |true |1        |foo   |(262144,[174475,247670,257907,262126],[2.2,1.0,1.0,1.0])|\n",
      "|3.3 |false|2        |bar   |(262144,[70644,89673,173866,174475],[1.0,1.0,1.0,3.3])  |\n",
      "|4.4 |false|3        |baz   |(262144,[22406,70644,174475,187923],[1.0,1.0,4.4,1.0])  |\n",
      "|5.5 |false|4        |foo   |(262144,[70644,101499,174475,257907],[1.0,1.0,5.5,1.0]) |\n",
      "+----+-----+---------+------+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.createDataFrame([ (2.2, True, \"1\", \"foo\"), (3.3, False, \"2\", \"bar\"), (4.4, False, \"3\", \"baz\"), (5.5, False, \"4\", \"foo\") ], [\"real\", \"bool\", \"stringNum\", \"string\"])\n",
    "\n",
    "fh = pmf.FeatureHasher(inputCols=[\"real\", \"bool\", \"stringNum\", \"string\"], outputCol=\"features\")\n",
    "\n",
    "df_hash = fh.transform(df)\n",
    "df_hash.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.createDataFrame([(0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]), (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])], [\"id\", \"raw\"])\n",
    "\n",
    "sw = pmf.StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "sw.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.createDataFrame([(0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),(1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),(2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])], [\"id\", \"words\"])\n",
    "\n",
    "ng = pmf.NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "df_ng = ng.transform(df)\n",
    "df_ng.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -797.5200544004555\n",
      "The upper bound on perplexity: 3.0673848246171365\n"
     ]
    }
   ],
   "source": [
    "path = 'D:/ProgramFiles/Spark/spark-3.0.0-bin-hadoop2.7/data/mllib/'\n",
    "df = sql.read.format(\"libsvm\").load(path + \"sample_lda_libsvm_data.txt\")\n",
    "\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "lda_fit = lda.fit(df)\n",
    "\n",
    "ll = lda_fit.logLikelihood(df)\n",
    "lp = lda_fit.logPerplexity(df)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[1, 3, 4]  |[0.10368082673401043, 0.10246920001021176, 0.09945342991888821]|\n",
      "|1    |[0, 5, 9]  |[0.1076176051626867, 0.09801051465962088, 0.09705006627909334] |\n",
      "|2    |[5, 10, 9] |[0.09817190617101527, 0.0981118296756393, 0.09564406780739915] |\n",
      "|3    |[5, 10, 2] |[0.10428733568856435, 0.10200642097504846, 0.09787247160826047]|\n",
      "|4    |[5, 8, 2]  |[0.10610350651837704, 0.10225089640708551, 0.0969920925809682] |\n",
      "|5    |[2, 1, 5]  |[0.1017814308587037, 0.09673789329662605, 0.09602700830858574] |\n",
      "|6    |[3, 5, 4]  |[0.1616436533169385, 0.1391919780709568, 0.11423150148553422]  |\n",
      "|7    |[8, 3, 5]  |[0.10449091906046457, 0.09702934195910436, 0.09685753582283695]|\n",
      "|8    |[2, 10, 5] |[0.20489639898500683, 0.0964903976654958, 0.09642582722801787] |\n",
      "|9    |[9, 1, 8]  |[0.10442114834534162, 0.0972521760982583, 0.09678634963629762] |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                     |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004676952344250045,0.004676968395495595,0.004676950578410848,0.004676970332583484,0.004676967419293877,0.0046769759336480555,0.00586884662388327,0.004676918960470165,0.9567155317134576,0.004676917698507178]     |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007805785111705882,0.007805628378410143,0.007805672976519763,0.007805633036666408,0.0078056512171248245,0.0078057068437358225,0.9296064359680699,0.007805670690530401,0.007948154376537068,0.0078056614006996585]  |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004065785807285062,0.004065796483945533,0.004065741735817013,0.004065754454676408,0.004065789124629428,0.004065756363747333,0.9633338428475929,0.004065766920250332,0.004140035607639875,0.004065730654416079]     |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.003595999611587824,0.0035959934673908856,0.003595986426016544,0.003596001942206489,0.0035960042711613415,0.0035959699075005618,0.9675706109074504,0.0035959527682424314,0.003661498769496744,0.0035959819289466694]|\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003896056145092278,0.0038960230822260213,0.003896021727591732,0.0038960395775907686,0.0038960305976958186,0.003896041370270874,0.9648646437935958,0.003896005371138772,0.003967131081227989,0.00389600725356977]   |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.0035958512635438883,0.003595871053475638,0.00359585118699201,0.0035958415673139612,0.0035958614858202405,0.00359585244224661,0.9675718310622518,0.003595862687536621,0.0036613294104646645,0.0035958478403545777]  |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.0037400901498766165,0.0037400772487206613,0.003740070441270972,0.003740090492304099,0.00374009199048886,0.003740054667297089,0.9662712134927176,0.0037400345829021095,0.0038082130712644995,0.0037400638631574046] |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004250912590410982,0.004250864880548274,0.004250873025195017,0.004250902392318307,0.004250896259490729,0.004250894608318159,0.9616644557550442,0.004250858331703485,0.004328484876898299,0.0042508572800724845]    |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004250932615766632,0.004250898540265684,0.0042508952107341985,0.00425086861721875,0.004250884020041738,0.004250912696613805,0.9616644057735709,0.004250906355026676,0.004328414896664478,0.004250881274096983]     |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.003223448053600673,0.0032234489591785147,0.0032234166852612113,0.003223426409605953,0.0032234482613675165,0.0032234389423863494,0.9709301695594706,0.003223426508496813,0.003282337187326747,0.003223439433305499] |\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004065743021552847,0.004065719209428452,0.004065721146442432,0.004065739269769889,0.004065742925579614,0.004065734100893753,0.9633342514366949,0.004065707081142349,0.004139929792297152,0.004065712016198601]     |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.0046768759999497155,0.00467689354556461,0.00467689400000143,0.00467684536293083,0.004676869638433628,0.004676885773237369,0.9578227095930838,0.004676890777371828,0.004762279954757638,0.004676855354669124]       |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe topics.\n",
    "topics = lda_fit.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = lda_fit.transform(df)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits & Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/latest/ml-classification-regression.html\n",
    "    http://spark.apache.org/docs/latest/ml-clustering.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bitc33de82c9da04edea88eb124459bf44a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
